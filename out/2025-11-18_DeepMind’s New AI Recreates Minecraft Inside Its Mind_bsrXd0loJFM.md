# DeepMind’s New AI Recreates Minecraft Inside Its Mind

- Channel: Two Minute Papers
- Published: 2025-11-18T18:37:55Z
- URL: https://www.youtube.com/watch?v=bsrXd0loJFM

---

概要（3-5行）
- DeepMindの新手法は、マインクラフトを一度もプレイせず、人間のプレイ映像のごく少量だけから「内部の世界モデル（小さなシミュレータ）」を学習し、その中で何百万回も“夢”のように練習して実際のゲームを高確率で成功させる。  
- これにより、OpenAIの大量データ（100倍）を使った手法を上回る成果を出し、石→鉄→ダイヤの取得など長い行動列の達成も可能になった。  
- 一方で、内部シミュレーションは短期的には精度が高いが長期では誤差が蓄積しやすく、短い夢を繋ぎ合わせることで長い行動を実現している点に限界がある。

主要トピック（箇条書き）
- 3段階の学習プロセス：世界モデル事前学習 → 想像内での価値学習（夢の強化学習） → 実行ポリシーへの適用  
- 少量データでの世界モデル構築（YouTube等の大量データやゲームアクセス不要）  
- 想像内プレイ（内的シミュレーション）による効率的な自己改良  
- 既存手法との比較：BC（行動模倣）、VLA/VPT（大量の映像＆ラベル）より効率的  
- 限界：短期予測は良好だが長期予測の誤差蓄積（ツリーが元に戻る等の現象）  
- 応用可能性：ロボットの安全な仮想トレーニングや現実世界でのwhat-if解析

重要ポイント（番号付き、順序に沿った説明）
1. 何をしたか（序盤）
   - 人間のプレイ映像のごく少量だけを与え、実際のゲーム環境には一切触れさせずに内部の世界モデルを学習した。  
2. 学習の流れ（中盤）
   - フェーズ1：世界モデル事前学習（映像から「どう動くか」を模倣する内部シミュレータを構築）  
   - フェーズ2：想像内で報酬（ブロック採掘など）を与え、内的シミュレーション上で行動の価値を学習（短い夢を多数実行）  
   - フェーズ3：夢で学んだことを実際のポリシーに適用してゲーム内で実行  
3. 成果（中〜終盤）
   - データ量はOpenAIの手法より約100倍少ないにもかかわらず、石・鉄・稀にダイヤ取得まで成功率が高く、従来手法を大きく上回るケースがある。  
4. 長期依存性の限界（終盤）
   - 一回の長い夢で長期因果を正確に予測するのではなく、短い夢を繋ぎ合わせて長い行動列を作る方式。短期は正確でも微小な誤差が累積しやすく、長時間の一貫した理解には弱い（例：倒した木がシミュレーション内で戻ってしまう）。  
5. 広い応用可能性（終盤）
   - Minecraftに限定されず、物理的what-ifシミュレーションやロボットの安全な仮想練習に適用できる潜在力がある。

アクション項目 / 役立つTips
- 研究者向け
  - 少量データでの世界モデル構築＋想像内強化学習は有望：データ効率化を狙う研究に適用する価値あり。  
  - 長期依存の改善には、階層化されたプランニング（長期目標を短期夢に分割）、モデル不確実性の推定（アンサンブルや不確実性ペナルティ）、オンラインでの実環境微調整を組み合わせる。  
  - ロボティクスでは「想像内での失敗→現実での安全確認」を必須にし、シミュレータと実環境のギャップを監視する。  
- 実装／プロダクト向け
  - 世界モデルは短期予測に強い設計を優先し、重要な長期条件は外部メモリや階層的方策で補う。  
  - シミュレーションの誤差が致命的な場面では、モデル不確実性に基づく保守的選択を導入する。  
- 一般向け
  - 詳細は論文を確認（DeepMindの論文を参照）し、続報・再現実験を注視すること。Two Minute Papersは要点把握に便利だが、技術的検証は原論文で行う。

英語 TL;DR (1-2 lines)
DeepMind trained a compact world model from a tiny amount of human gameplay, then practiced inside that internal simulator to outperform prior Minecraft agents using ~100x less data — it excels short-term but struggles with long-horizon model error accumulation.
