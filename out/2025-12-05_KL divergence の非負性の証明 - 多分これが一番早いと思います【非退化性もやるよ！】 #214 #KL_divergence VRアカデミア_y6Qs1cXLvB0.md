# KL divergence の非負性の証明 - 多分これが一番早いと思います【非退化性もやるよ！】 #214 #KL_divergence VRアカデミア

- Channel: AIcia Solid Project
- Published: 2025-12-05T11:00:06Z
- URL: https://www.youtube.com/watch?v=y6Qs1cXLvB0

---

概要（3-5行）
- KLダイバージェンス KL(P||Q) = ∫ p(x) log(p(x)/q(x)) dx が常に0以上であり、0のときはほぼ至る所でP＝Q（非退化性）であることを最短で示す証明動画です。  
- 証明の要は相加相乗平均の不等式（＝対数を使った期待値の不等式、Jensen的な主張）で、KLを期待値に直してその不等号を当てはめるだけで結論が出ます。  
- 実務的な注意としては、比 p/q が定義される（QがPのサポートを覆っている）ことが前提になります。

主要トピック（箇条書き）
- KLダイバージェンスの定義（連続分布・確率密度の場合）
- 相加相乗平均の不等式（AM ≥ GM）とそのログ版
- 期待値とログの順序（log(E[X]) ≥ E[log X]：Jensenに相当）
- KL ≥ 0 の簡潔な導出
- 等号成立条件（非退化性）：KL = 0 ⇔ P = Q（ほぼ至る所）

重要ポイント（番号付き、簡易タイムスタンプ付き）
1. (00:00–00:30) 目的：KL(P||Q) ≥ 0 と KL(P||Q)=0 のとき P=Q を示す。  
2. (00:30–01:10) 定義：連続の場合、KL(P||Q) = ∫ p(x) log(p(x)/q(x)) dx = E_p[ log(p/q) ]。  
3. (01:10–02:30) キー不等式：相加相乗平均の不等式をログで見ると、log(E[X]) ≥ E[log X] が得られる（一般化して期待値版も成立）。  
4. (02:30–03:30) 証明の流れ：KL = E_p[log(p/q)] = −E_p[log(q/p)]。ここで X = q/p に対して log(E_p[X]) ≥ E_p[log X] を用いると −E_p[log(q/p)] ≥ −log(E_p[q/p]) = −log(∫ q dx) = −log 1 = 0。よって KL ≥ 0。  
5. (03:30–04:30) 等号条件：Jensen/AM-GMで等号が成り立つのは対象がほぼ確実に定数のとき。ここでは q/p が定数 ⇒ 両方の積分が1であることからその定数は1 ⇒ p=q（a.e.）。  
6. (補足) 技術的条件：比 p/q が定義できない点（q(x)=0 かつ p(x)>0）があるとKLは +∞ になるので、証明では通常 Q が P のサポートを覆うことを仮定する。

アクション項目 / 役立つTips
- KLを扱うときはまず「期待値」の形 E_p[·] に書き換えると不等式適用が簡単になる。  
- logは凹関数なので Jensen の形（log(E[X]) ≥ E[log X]）を使えば同様の証明が短くできる。  
- 等号条件を調べるときは「不等号が成り立つときの定数化条件（すべて同じ値になる）」を確認する習慣をつけるとよい。  
- 実務ではサポートの扱いに注意：Qが0のところにPが質量を持つとKLは無限大になる（比の定義域をチェック）。

英語 TL;DR
KL(P||Q) ≥ 0 is proven by writing KL as an expectation E_p[log(p/q)] and applying log(E[X]) ≥ E[log X] (AM–GM / Jensen). Equality holds iff P = Q almost everywhere.
